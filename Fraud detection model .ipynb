{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5066eb05-c51e-4e24-acfe-86858a82c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\91704\\anaconda3\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\91704\\anaconda3\\lib\\site-packages (from SpeechRecognition) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\91704\\anaconda3\\lib\\site-packages (from SpeechRecognition) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91704\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91704\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91704\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91704\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050e539e-c6be-42e4-80df-c071f3d31b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91704\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91704\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re              #package for importing regular expression\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import speech_recognition as sr\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1260114d-5d7c-41c6-8981-f96ac5c6aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text():\n",
    "    conv=[]\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Listening stopped\")\n",
    "    try:  \n",
    "        text = recognizer.recognize_google(audio)\n",
    "        conv.append(text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, I could not understand.\")\n",
    "    return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444c4655-0c40-4588-bbd5-c676b3e90645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to preprocess the data\n",
    "def preprocessing(dataset,num_of_rows=1):\n",
    "    stemmer =WordNetLemmatizer()\n",
    "    corpus=[]\n",
    "    #nltk.download('wordnet')\n",
    "    for i in range(0,num_of_rows):\n",
    "        #Removing words which are special character\n",
    "        document=re.sub(r'\\W',' ',dataset[i])\n",
    "        \n",
    "    \n",
    "        #Removing single characters from the document\n",
    "        document=re.sub(r'\\s+[a-zA-Z]\\s+',' ',document)\n",
    "    \n",
    "        #Removing single character from start\n",
    "        document=re.sub(r'\\^[a-zA-Z]\\s+',' ',document)\n",
    "    \n",
    "        #Removing one or more spaces and replacing by one space\n",
    "        document=re.sub(r'\\s+',' ',document,flags=re.I)\n",
    "    \n",
    "        document=document.lower()\n",
    "    \n",
    "        document=document.split()\n",
    "        document=[stemmer.lemmatize(w) for w in document]\n",
    "        document=' '.join(document)\n",
    "    \n",
    "        #Now adding it to our corpus\n",
    "        corpus.append(document)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7faf09eb-798c-4a4e-b9b3-7db90b4df6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to train model 1\n",
    "def predict_using_count_vectoriser(dataset,num_of_rows,callData):\n",
    "    #Now we will divide the data for train and test our dataset\n",
    "    x=preprocessing(dataset['call_content'],num_of_rows)\n",
    "    y=dataset.Label   \n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n",
    "    #Testing the model over given conversation\n",
    "    #callData=[]\n",
    "    #callData=audio_to_text()\n",
    "    #callData=[\"Please don't text me anymore.\"]\n",
    "    live_test=preprocessing(callData,len(callData))\n",
    "    \n",
    "    # tokenizing the text data and counting the occurrences of each token using CountVectorizer\n",
    "    count_vectorizer=CountVectorizer(max_features=1500,min_df=5,max_df=0.7,stop_words=stopwords.words('english'))\n",
    "    \n",
    "    #fits the vectorizer to the documents (learns the vocabulary) and transforms the documents into matrix\n",
    "    count_train=count_vectorizer.fit_transform(x_train)\n",
    "    #only transform the document into matrix\n",
    "    count_test=count_vectorizer.transform(x_test)\n",
    "    live_count_test=count_vectorizer.transform(live_test)\n",
    "    \n",
    "    # Now with the help of Naive Bayes theorm we will fit the label\n",
    "    nbclassifier=MultinomialNB()\n",
    "    nbclassifier.fit(count_train,y_train)\n",
    "    y_pred=nbclassifier.predict(count_test)\n",
    "    live_y_pred=nbclassifier.predict(live_count_test)\n",
    "    score=metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "    cm=metrics.confusion_matrix(y_pred,y_test,labels=['normal','fraud'])\n",
    "    print(\"Accuracy score when using count vectoriser class: \",score)\n",
    "    print(\"CONFUSION MATRIX\\n\",cm)\n",
    "    return callData,live_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a2e83-fff9-4176-aa2b-b4ecf3e21b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main calling\n",
    "# To load training dataset\n",
    "dataset=pd.read_csv(r\"Fraud_calls.txt\",sep='|')\n",
    "#dataset.head()\n",
    "num_of_rows,y=dataset.shape\n",
    "callData=audio_to_text()\n",
    "\n",
    "print(\"\\n ********** USING COUNT VECTORISER **********\\n\")\n",
    "conversation,label=predict_using_count_vectoriser(dataset,num_of_rows,callData)\n",
    "print(\"Audio Received:{} \\npredicted as label:{}\".format(conversation,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1773e-01f0-4576-9db0-43dc189b5fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c9f02-11d1-4751-9d3f-15a100bb75ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
